apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nvlink-test-daemonset
  labels:
    app: nvlink-test
spec:
  selector:
    matchLabels:
      app: nvlink-test
  template:
    metadata:
      labels:
        app: nvlink-test
    spec:
      restartPolicy: Always
      containers:
        - name: nvlink-test-container
          image: nvidia/cuda:12.2.0-base-ubuntu22.04  # Ensure it includes necessary tools
          command: ["/bin/bash", "-c"]
          args:
            - |
              set -euo pipefail
              
              # Handle errors
              error_handler() {
                  echo "Error on line $1"
                  # Optionally, implement notification or logging
              }
              trap 'error_handler $LINENO' ERR
              
              # Install necessary tools
              apt-get update && apt-get install -y git build-essential wget
              
              # Install NCCL Tests
              if ! command -v all_reduce_perf &> /dev/null; then
                  echo "Installing NCCL Tests..."
                  git clone https://github.com/NVIDIA/nccl-tests.git /tmp/nccl-tests
                  cd /tmp/nccl-tests
                  make
                  export PATH=$PATH:/tmp/nccl-tests
              fi
              
              # Install Prometheus Node Exporter (optional)
              if ! command -v node_exporter &> /dev/null; then
                  echo "Installing Prometheus Node Exporter..."
                  wget https://github.com/prometheus/node_exporter/releases/download/v1.5.0/node_exporter-1.5.0.linux-amd64.tar.gz
                  tar xzf node_exporter-1.5.0.linux-amd64.tar.gz
                  mv node_exporter-1.5.0.linux-amd64/node_exporter /usr/local/bin/
                  nohup node_exporter &> /dev/null &
              fi
              
              # Collect environment variables
              NODE_NAME=${NODE_NAME:-unknown}
              POD_NAME=${POD_NAME:-unknown}
              POD_NAMESPACE=${POD_NAMESPACE:-unknown}
              POD_IP=${POD_IP:-unknown}
              HOST_IP=${HOST_IP:-unknown}
              TEST_DURATION=${TEST_DURATION:-60}
              NCCL_TEST_ITERATIONS=${NCCL_TEST_ITERATIONS:-10}
              
              echo "=== Kubernetes & Environment Variables ==="
              echo "Node Name:       ${NODE_NAME}"
              echo "Pod Name:        ${POD_NAME}"
              echo "Pod Namespace:   ${POD_NAMESPACE}"
              echo "Pod IP:          ${POD_IP}"
              echo "Host IP:         ${HOST_IP}"
              echo "=========================================="
              echo
              
              echo "=== System & NVIDIA Driver Info ==="
              uname -a
              nvidia-smi --query-gpu=name,index,pci.bus_id --format=csv,noheader || {
                  echo "Error: nvidia-smi not found or no GPUs detected."
                  exit 1
              }
              echo "===================================="
              echo
              
              # Collect detailed NVLink metrics
              echo "=== Detailed NVLink Metrics ==="
              nvidia-smi nvlink -q -d PERF
              nvidia-smi nvlink -q -d ERROR
              echo "=============================="
              echo
              
              # Collect GPUs
              ALL_GPUS=$(nvidia-smi --query-gpu=index --format=csv,noheader)
              echo "GPUs found: ${ALL_GPUS}"
              echo
              
              declare -a RESULTS
              
              for GPU_ID in ${ALL_GPUS}; do
                  echo "---- GPU ${GPU_ID} ----"
                  
                  for LINK_ID in 0 1 2 3 4 5; do
                      LINK_STATUS=$(nvidia-smi nvlink -i ${GPU_ID} -g ${LINK_ID} -q 2>&1 | grep -i "Link" || true)
                      if [[ -z "$LINK_STATUS" ]]; then
                          continue
                      fi
                      
                      echo "Link ${LINK_ID}:"
                      
                      if ! nvidia-smi nvlink -i ${GPU_ID} -g ${LINK_ID} -p; then
                          echo "  (Reset not supported or failed for Link ${LINK_ID})"
                      else
                          echo "  Reset NVLink counters successfully."
                      fi
                      
                      sleep 1
                      
                      if ! nvidia-smi nvlink -i ${GPU_ID} -g ${LINK_ID} -c; then
                          echo "  (Reading counters not supported or failed for Link ${LINK_ID})"
                      else
                          echo "  Successfully read NVLink counters."
                      fi
                      
                      # Collect metrics
                      LINK_METRICS=$(nvidia-smi nvlink -i ${GPU_ID} -g ${LINK_ID} -q)
                      RESULTS+=("{\"GPU_ID\": ${GPU_ID}, \"LINK_ID\": ${LINK_ID}, \"Metrics\": \"${LINK_METRICS//\"/\\\"}\"}")
                      
                      echo
                  done
                  
                  # Run NCCL bandwidth test between GPU pairs
                  # This assumes at least two GPUs per node
                  PEER_GPUS=$(nvidia-smi topo -m | grep "NV1" | awk '{print $1}')
                  # Simplistic peer GPU identification
                  for PEER_GPU in ${ALL_GPUS}; do
                      if [[ "${PEER_GPU}" != "${GPU_ID}" ]]; then
                          echo "Running NCCL Bandwidth Test between GPU ${GPU_ID} and GPU ${PEER_GPU}"
                          ./build/all_reduce_perf -b 8 -e 512M -f 2 -g 2 -t ${TEST_DURATION} -n ${NCCL_TEST_ITERATIONS}
                          echo "NCCL Test between GPU ${GPU_ID} and GPU ${PEER_GPU} completed."
                      fi
                  done
                  
                  echo
              done
              
              # Save results in JSON
              save_results_json() {
                  local results_file="/tmp/nvlink_results.json"
                  echo "[" > ${results_file}
                  local first=true
                  for result in "${RESULTS[@]}"; do
                      if [ "${first}" = true ]; then
                          first=false
                      else
                          echo "," >> ${results_file}
                      fi
                      echo "${result}" >> ${results_file}
                  done
                  echo "]" >> ${results_file}
                  echo "NVLink results saved to ${results_file}"
              }
              
              save_results_json
              
              # Export metrics (optional)
              # This is a placeholder. Implement Prometheus exporter as needed.
              
              # Keep the container alive
              echo "NVLink test completed. Waiting for termination signal."
              while true; do
                  sleep 3600
              done
          env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: TEST_DURATION
              value: "60"  # Duration in seconds
            - name: NCCL_TEST_ITERATIONS
              value: "10"
          securityContext:
            runAsUser: 0  # May be required for certain operations
          volumeMounts:
            - name: nvidia-driver
              mountPath: /usr/local/nvidia
      volumes:
        - name: nvidia-driver
          hostPath:
            path: /usr/local/nvidia
            type: Directory
      tolerations:
        - effect: NoSchedule
          operator: Exists
